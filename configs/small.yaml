model:
  attention: OmnidirectionalAttention
  depth: 8
  conv_kernel_size: 11
  weight_shared_blocks: 1
  batch_size: 1
  features: 2048
  feed_forward_intermediate_factor: 0.125
optimizer:
  beta2: 0.95
  gradient_accumulation_steps: 1
  one_cycle:
    total_steps: 8192
    max_lr: 0.01
log:
  loss_steps_per_print: 8
dataset:
  num_workers: 12
