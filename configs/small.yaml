model:
  attention: FFTAttention
  sequence_length: 2048
  omnidirectional: no
  depth: 16
  conv_kernel_size: 11
  weight_shared_blocks: 1
  batch_size: 8
  offloading: yes
  features: 2048
  feed_forward_intermediate_factor: 0.125
optimizer:
  beta2: 0.95
  gradient_accumulation_steps: 1
log:
  loss_steps_per_print: 4
dataset:
  num_workers: 16
